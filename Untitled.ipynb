{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5657d9-1227-4163-8db3-5e6289f23a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder_only_gpt import My_GPT_model\n",
    "def build_model(config):\n",
    "    model = My_GPT_model(\n",
    "        vocab_size=config[\"model\"][\"vocab_size\"],\n",
    "        d_model=config[\"model\"][\"d_model\"],\n",
    "        num_layers=config[\"model\"][\"n_layer\"],\n",
    "        num_heads=config[\"model\"][\"n_head\"],\n",
    "        d_ff=config[\"model\"][\"d_ff\"],\n",
    "        seq_len=config[\"model\"][\"seq_len\"],\n",
    "        dropout=config[\"model\"][\"dropout\"]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f506e824-4802-43e2-a781-dc09ecb8d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # --------------------\n",
    "    # Model architecture\n",
    "    # --------------------\n",
    "    \"model\": {\n",
    "        \"vocab_size\": 32768,\n",
    "        \"d_model\": 512,\n",
    "        \"n_layer\": 12,\n",
    "        \"n_head\": 8,\n",
    "        \"d_ff\": 2048,\n",
    "        \"seq_len\": 512,\n",
    "        \"dropout\": 0.1,\n",
    "        \"weight_tying\": True,\n",
    "        \"norm_type\": \"rmsnorm\",\n",
    "        \"ffn_type\": \"swiglu\"\n",
    "    },\n",
    "\n",
    "    # --------------------\n",
    "    # Training (SFT)\n",
    "    # --------------------\n",
    "    \"train\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"micro_batch_size\": 1,     # future gradient accumulation\n",
    "        \"grad_accum_steps\": 1,\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": 1e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"label_smoothing\": 0.0,\n",
    "        \"ignore_index\": -100,\n",
    "        \"fp16\": True,              # future ready\n",
    "        \"bf16\": False,\n",
    "        \"seed\": 42\n",
    "    },\n",
    "\n",
    "    # --------------------\n",
    "    # Data\n",
    "    # --------------------\n",
    "    \"data\": {\n",
    "        \"dataset\": \"hindi_sft_v1\",\n",
    "        \"format\": \"### प्रश्न: / ### उत्तर:\",\n",
    "        \"pad_token_id\": 0,\n",
    "        \"bos_token_id\": 2, \n",
    "        \"eos_token_id\": 3,\n",
    "        \"max_seq_len\": 512,\n",
    "        \"mask_prompt\": True        # loss only on answer\n",
    "    },\n",
    "\n",
    "    # --------------------\n",
    "    # Logging / Checkpoint\n",
    "    # --------------------\n",
    "    \"logging\": {\n",
    "        \"project\": \"HindiGPT-SFT\",\n",
    "        \"log_every\": 50,\n",
    "        \"eval_every\": 2000,\n",
    "        \"save_every\": 7000,\n",
    "        \"save_dir\": \"checkpoints_sft\",\n",
    "        \"wandb\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "base_model = build_model(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb09596f-4e25-459b-8a00-e1cc9bd42548",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = My_GPT_model(vocab_size=CONFIG[\"model\"][\"vocab_size\"],\n",
    "        d_model=CONFIG[\"model\"][\"d_model\"],\n",
    "        num_layers=CONFIG[\"model\"][\"n_layer\"],\n",
    "        num_heads=CONFIG[\"model\"][\"n_head\"],\n",
    "        d_ff=CONFIG[\"model\"][\"d_ff\"],\n",
    "        seq_len=CONFIG[\"model\"][\"seq_len\"],\n",
    "        dropout=CONFIG[\"model\"][\"dropout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07b4215-9875-4717-9d90-730f9f68ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"checkpoints_HindiGPT-v1_step280000.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "800ed486-7039-447e-a010-3ef1931bebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_state = ckpt[\"model\"]\n",
    "\n",
    "clean_state = {}\n",
    "for k, v in raw_state.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        k = k.replace(\"_orig_mod.\", \"\")\n",
    "    clean_state[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c667a394-a090-4d9f-9eba-0a5f3bc57910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decoder.causal_mask', 'decoder.embedding.weight', 'decoder.layers.0.swi_glu.w1.weight', 'decoder.layers.0.swi_glu.w2.weight', 'decoder.layers.0.swi_glu.w3.weight', 'decoder.layers.0.masked_mha.Q.weight', 'decoder.layers.0.masked_mha.Q.bias', 'decoder.layers.0.masked_mha.K.weight', 'decoder.layers.0.masked_mha.K.bias', 'decoder.layers.0.masked_mha.V.weight', 'decoder.layers.0.masked_mha.V.bias', 'decoder.layers.0.masked_mha.fc_out.weight', 'decoder.layers.0.masked_mha.fc_out.bias', 'decoder.layers.0.rms_norm0.weight', 'decoder.layers.0.rms_norm1.weight', 'decoder.layers.1.swi_glu.w1.weight', 'decoder.layers.1.swi_glu.w2.weight', 'decoder.layers.1.swi_glu.w3.weight', 'decoder.layers.1.masked_mha.Q.weight', 'decoder.layers.1.masked_mha.Q.bias']\n"
     ]
    }
   ],
   "source": [
    "state_dict = ckpt[\"model\"]\n",
    "print(list(clean_state.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb5ea4f-ad71-4cd3-a1c3-f7e3e71672d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrain_model.load_state_dict(clean_state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2ec6de-de20-437f-a06f-b14c6bc99e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pretrain_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5db5bf25-f988-4e87-b8f5-18fc92076455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_linear: nn.Linear, r=16, alpha=16, dropout=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = base_linear.weight\n",
    "        self.weight.requires_grad = False\n",
    "        self.bias = base_linear.bias\n",
    "\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        self.lora_A = nn.Linear(base_linear.in_features, r, bias=False)\n",
    "        self.lora_B = nn.Linear(r, base_linear.out_features, bias=False)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.linear(x, self.weight, self.bias)\n",
    "        out += self.lora_B(self.lora_A(self.dropout(x))) * self.scaling\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf2651d4-de3e-42ff-92d0-72a67f3a6fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora_attention(model, r=8, alpha=16):\n",
    "    for layer in model.decoder.layers:\n",
    "        mha = layer.masked_mha\n",
    "\n",
    "        mha.Q = LoRALinear(mha.Q, r=r, alpha=alpha)\n",
    "        mha.K = LoRALinear(mha.K, r=r, alpha=alpha)\n",
    "        mha.V = LoRALinear(mha.V, r=r, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc6ff70-7aac-4145-9d53-2441f0078df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "inject_lora_attention(pretrain_model, r=8, alpha=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da10904-758e-444e-97e8-0db101211e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "trainable = [(n, p.shape) for n, p in pretrain_model.named_parameters() if p.requires_grad]\n",
    "print(len(trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a72db2-157f-4c69-a663-6858ff9b0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(\"lora_\" in n for n, p in pretrain_model.named_parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b89a4da-8ced-4b7f-bd08-70e7738c5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = [p for p in pretrain_model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lora_params,\n",
    "    lr=2e-4,\n",
    "    weight_decay=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1b6bc0-9582-44e1-b0b7-ff008b562921",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_state = {\n",
    "    k: v.cpu()\n",
    "    for k, v in pretrain_model.state_dict().items()\n",
    "    if \"lora_\" in k\n",
    "}\n",
    "\n",
    "torch.save(lora_state, \"lora_sft.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ce49339-b587-46d3-98eb-e5e026c6e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_lora_attention(model, r=8, alpha=16):\n",
    "    for layer in model.decoder.layers:\n",
    "        mha = layer.masked_mha\n",
    "\n",
    "        if not isinstance(mha.Q, LoRALinear):\n",
    "            mha.Q = LoRALinear(mha.Q, r=r, alpha=alpha)\n",
    "\n",
    "        if not isinstance(mha.K, LoRALinear):\n",
    "            mha.K = LoRALinear(mha.K, r=r, alpha=alpha)\n",
    "\n",
    "        if not isinstance(mha.V, LoRALinear):\n",
    "            mha.V = LoRALinear(mha.V, r=r, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf64e60-b4f5-4b5b-a371-25c11a74b5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['decoder.causal_mask', 'decoder.embedding.weight', 'decoder.layers.0.swi_glu.w1.weight', 'decoder.layers.0.swi_glu.w2.weight', 'decoder.layers.0.swi_glu.w3.weight', 'decoder.layers.0.masked_mha.Q.weight', 'decoder.layers.0.masked_mha.Q.bias', 'decoder.layers.0.masked_mha.K.weight', 'decoder.layers.0.masked_mha.K.bias', 'decoder.layers.0.masked_mha.V.weight', 'decoder.layers.0.masked_mha.V.bias', 'decoder.layers.0.masked_mha.fc_out.weight', 'decoder.layers.0.masked_mha.fc_out.bias', 'decoder.layers.0.rms_norm0.weight', 'decoder.layers.0.rms_norm1.weight', 'decoder.layers.1.swi_glu.w1.weight', 'decoder.layers.1.swi_glu.w2.weight', 'decoder.layers.1.swi_glu.w3.weight', 'decoder.layers.1.masked_mha.Q.weight', 'decoder.layers.1.masked_mha.Q.bias', 'decoder.layers.1.masked_mha.K.weight', 'decoder.layers.1.masked_mha.K.bias', 'decoder.layers.1.masked_mha.V.weight', 'decoder.layers.1.masked_mha.V.bias', 'decoder.layers.1.masked_mha.fc_out.weight', 'decoder.layers.1.masked_mha.fc_out.bias', 'decoder.layers.1.rms_norm0.weight', 'decoder.layers.1.rms_norm1.weight', 'decoder.layers.2.swi_glu.w1.weight', 'decoder.layers.2.swi_glu.w2.weight', 'decoder.layers.2.swi_glu.w3.weight', 'decoder.layers.2.masked_mha.Q.weight', 'decoder.layers.2.masked_mha.Q.bias', 'decoder.layers.2.masked_mha.K.weight', 'decoder.layers.2.masked_mha.K.bias', 'decoder.layers.2.masked_mha.V.weight', 'decoder.layers.2.masked_mha.V.bias', 'decoder.layers.2.masked_mha.fc_out.weight', 'decoder.layers.2.masked_mha.fc_out.bias', 'decoder.layers.2.rms_norm0.weight', 'decoder.layers.2.rms_norm1.weight', 'decoder.layers.3.swi_glu.w1.weight', 'decoder.layers.3.swi_glu.w2.weight', 'decoder.layers.3.swi_glu.w3.weight', 'decoder.layers.3.masked_mha.Q.weight', 'decoder.layers.3.masked_mha.Q.bias', 'decoder.layers.3.masked_mha.K.weight', 'decoder.layers.3.masked_mha.K.bias', 'decoder.layers.3.masked_mha.V.weight', 'decoder.layers.3.masked_mha.V.bias', 'decoder.layers.3.masked_mha.fc_out.weight', 'decoder.layers.3.masked_mha.fc_out.bias', 'decoder.layers.3.rms_norm0.weight', 'decoder.layers.3.rms_norm1.weight', 'decoder.layers.4.swi_glu.w1.weight', 'decoder.layers.4.swi_glu.w2.weight', 'decoder.layers.4.swi_glu.w3.weight', 'decoder.layers.4.masked_mha.Q.weight', 'decoder.layers.4.masked_mha.Q.bias', 'decoder.layers.4.masked_mha.K.weight', 'decoder.layers.4.masked_mha.K.bias', 'decoder.layers.4.masked_mha.V.weight', 'decoder.layers.4.masked_mha.V.bias', 'decoder.layers.4.masked_mha.fc_out.weight', 'decoder.layers.4.masked_mha.fc_out.bias', 'decoder.layers.4.rms_norm0.weight', 'decoder.layers.4.rms_norm1.weight', 'decoder.layers.5.swi_glu.w1.weight', 'decoder.layers.5.swi_glu.w2.weight', 'decoder.layers.5.swi_glu.w3.weight', 'decoder.layers.5.masked_mha.Q.weight', 'decoder.layers.5.masked_mha.Q.bias', 'decoder.layers.5.masked_mha.K.weight', 'decoder.layers.5.masked_mha.K.bias', 'decoder.layers.5.masked_mha.V.weight', 'decoder.layers.5.masked_mha.V.bias', 'decoder.layers.5.masked_mha.fc_out.weight', 'decoder.layers.5.masked_mha.fc_out.bias', 'decoder.layers.5.rms_norm0.weight', 'decoder.layers.5.rms_norm1.weight', 'decoder.layers.6.swi_glu.w1.weight', 'decoder.layers.6.swi_glu.w2.weight', 'decoder.layers.6.swi_glu.w3.weight', 'decoder.layers.6.masked_mha.Q.weight', 'decoder.layers.6.masked_mha.Q.bias', 'decoder.layers.6.masked_mha.K.weight', 'decoder.layers.6.masked_mha.K.bias', 'decoder.layers.6.masked_mha.V.weight', 'decoder.layers.6.masked_mha.V.bias', 'decoder.layers.6.masked_mha.fc_out.weight', 'decoder.layers.6.masked_mha.fc_out.bias', 'decoder.layers.6.rms_norm0.weight', 'decoder.layers.6.rms_norm1.weight', 'decoder.layers.7.swi_glu.w1.weight', 'decoder.layers.7.swi_glu.w2.weight', 'decoder.layers.7.swi_glu.w3.weight', 'decoder.layers.7.masked_mha.Q.weight', 'decoder.layers.7.masked_mha.Q.bias', 'decoder.layers.7.masked_mha.K.weight', 'decoder.layers.7.masked_mha.K.bias', 'decoder.layers.7.masked_mha.V.weight', 'decoder.layers.7.masked_mha.V.bias', 'decoder.layers.7.masked_mha.fc_out.weight', 'decoder.layers.7.masked_mha.fc_out.bias', 'decoder.layers.7.rms_norm0.weight', 'decoder.layers.7.rms_norm1.weight', 'decoder.layers.8.swi_glu.w1.weight', 'decoder.layers.8.swi_glu.w2.weight', 'decoder.layers.8.swi_glu.w3.weight', 'decoder.layers.8.masked_mha.Q.weight', 'decoder.layers.8.masked_mha.Q.bias', 'decoder.layers.8.masked_mha.K.weight', 'decoder.layers.8.masked_mha.K.bias', 'decoder.layers.8.masked_mha.V.weight', 'decoder.layers.8.masked_mha.V.bias', 'decoder.layers.8.masked_mha.fc_out.weight', 'decoder.layers.8.masked_mha.fc_out.bias', 'decoder.layers.8.rms_norm0.weight', 'decoder.layers.8.rms_norm1.weight', 'decoder.layers.9.swi_glu.w1.weight', 'decoder.layers.9.swi_glu.w2.weight', 'decoder.layers.9.swi_glu.w3.weight', 'decoder.layers.9.masked_mha.Q.weight', 'decoder.layers.9.masked_mha.Q.bias', 'decoder.layers.9.masked_mha.K.weight', 'decoder.layers.9.masked_mha.K.bias', 'decoder.layers.9.masked_mha.V.weight', 'decoder.layers.9.masked_mha.V.bias', 'decoder.layers.9.masked_mha.fc_out.weight', 'decoder.layers.9.masked_mha.fc_out.bias', 'decoder.layers.9.rms_norm0.weight', 'decoder.layers.9.rms_norm1.weight', 'decoder.layers.10.swi_glu.w1.weight', 'decoder.layers.10.swi_glu.w2.weight', 'decoder.layers.10.swi_glu.w3.weight', 'decoder.layers.10.masked_mha.Q.weight', 'decoder.layers.10.masked_mha.Q.bias', 'decoder.layers.10.masked_mha.K.weight', 'decoder.layers.10.masked_mha.K.bias', 'decoder.layers.10.masked_mha.V.weight', 'decoder.layers.10.masked_mha.V.bias', 'decoder.layers.10.masked_mha.fc_out.weight', 'decoder.layers.10.masked_mha.fc_out.bias', 'decoder.layers.10.rms_norm0.weight', 'decoder.layers.10.rms_norm1.weight', 'decoder.layers.11.swi_glu.w1.weight', 'decoder.layers.11.swi_glu.w2.weight', 'decoder.layers.11.swi_glu.w3.weight', 'decoder.layers.11.masked_mha.Q.weight', 'decoder.layers.11.masked_mha.Q.bias', 'decoder.layers.11.masked_mha.K.weight', 'decoder.layers.11.masked_mha.K.bias', 'decoder.layers.11.masked_mha.V.weight', 'decoder.layers.11.masked_mha.V.bias', 'decoder.layers.11.masked_mha.fc_out.weight', 'decoder.layers.11.masked_mha.fc_out.bias', 'decoder.layers.11.rms_norm0.weight', 'decoder.layers.11.rms_norm1.weight', 'decoder.norm.weight', 'lm_head.weight'], unexpected_keys=['decoder.layers.0.masked_mha.Q.lora_A.weight', 'decoder.layers.0.masked_mha.Q.lora_B.weight', 'decoder.layers.0.masked_mha.K.lora_A.weight', 'decoder.layers.0.masked_mha.K.lora_B.weight', 'decoder.layers.0.masked_mha.V.lora_A.weight', 'decoder.layers.0.masked_mha.V.lora_B.weight', 'decoder.layers.1.masked_mha.Q.lora_A.weight', 'decoder.layers.1.masked_mha.Q.lora_B.weight', 'decoder.layers.1.masked_mha.K.lora_A.weight', 'decoder.layers.1.masked_mha.K.lora_B.weight', 'decoder.layers.1.masked_mha.V.lora_A.weight', 'decoder.layers.1.masked_mha.V.lora_B.weight', 'decoder.layers.2.masked_mha.Q.lora_A.weight', 'decoder.layers.2.masked_mha.Q.lora_B.weight', 'decoder.layers.2.masked_mha.K.lora_A.weight', 'decoder.layers.2.masked_mha.K.lora_B.weight', 'decoder.layers.2.masked_mha.V.lora_A.weight', 'decoder.layers.2.masked_mha.V.lora_B.weight', 'decoder.layers.3.masked_mha.Q.lora_A.weight', 'decoder.layers.3.masked_mha.Q.lora_B.weight', 'decoder.layers.3.masked_mha.K.lora_A.weight', 'decoder.layers.3.masked_mha.K.lora_B.weight', 'decoder.layers.3.masked_mha.V.lora_A.weight', 'decoder.layers.3.masked_mha.V.lora_B.weight', 'decoder.layers.4.masked_mha.Q.lora_A.weight', 'decoder.layers.4.masked_mha.Q.lora_B.weight', 'decoder.layers.4.masked_mha.K.lora_A.weight', 'decoder.layers.4.masked_mha.K.lora_B.weight', 'decoder.layers.4.masked_mha.V.lora_A.weight', 'decoder.layers.4.masked_mha.V.lora_B.weight', 'decoder.layers.5.masked_mha.Q.lora_A.weight', 'decoder.layers.5.masked_mha.Q.lora_B.weight', 'decoder.layers.5.masked_mha.K.lora_A.weight', 'decoder.layers.5.masked_mha.K.lora_B.weight', 'decoder.layers.5.masked_mha.V.lora_A.weight', 'decoder.layers.5.masked_mha.V.lora_B.weight', 'decoder.layers.6.masked_mha.Q.lora_A.weight', 'decoder.layers.6.masked_mha.Q.lora_B.weight', 'decoder.layers.6.masked_mha.K.lora_A.weight', 'decoder.layers.6.masked_mha.K.lora_B.weight', 'decoder.layers.6.masked_mha.V.lora_A.weight', 'decoder.layers.6.masked_mha.V.lora_B.weight', 'decoder.layers.7.masked_mha.Q.lora_A.weight', 'decoder.layers.7.masked_mha.Q.lora_B.weight', 'decoder.layers.7.masked_mha.K.lora_A.weight', 'decoder.layers.7.masked_mha.K.lora_B.weight', 'decoder.layers.7.masked_mha.V.lora_A.weight', 'decoder.layers.7.masked_mha.V.lora_B.weight', 'decoder.layers.8.masked_mha.Q.lora_A.weight', 'decoder.layers.8.masked_mha.Q.lora_B.weight', 'decoder.layers.8.masked_mha.K.lora_A.weight', 'decoder.layers.8.masked_mha.K.lora_B.weight', 'decoder.layers.8.masked_mha.V.lora_A.weight', 'decoder.layers.8.masked_mha.V.lora_B.weight', 'decoder.layers.9.masked_mha.Q.lora_A.weight', 'decoder.layers.9.masked_mha.Q.lora_B.weight', 'decoder.layers.9.masked_mha.K.lora_A.weight', 'decoder.layers.9.masked_mha.K.lora_B.weight', 'decoder.layers.9.masked_mha.V.lora_A.weight', 'decoder.layers.9.masked_mha.V.lora_B.weight', 'decoder.layers.10.masked_mha.Q.lora_A.weight', 'decoder.layers.10.masked_mha.Q.lora_B.weight', 'decoder.layers.10.masked_mha.K.lora_A.weight', 'decoder.layers.10.masked_mha.K.lora_B.weight', 'decoder.layers.10.masked_mha.V.lora_A.weight', 'decoder.layers.10.masked_mha.V.lora_B.weight', 'decoder.layers.11.masked_mha.Q.lora_A.weight', 'decoder.layers.11.masked_mha.Q.lora_B.weight', 'decoder.layers.11.masked_mha.K.lora_A.weight', 'decoder.layers.11.masked_mha.K.lora_B.weight', 'decoder.layers.11.masked_mha.V.lora_A.weight', 'decoder.layers.11.masked_mha.V.lora_B.weight'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inject_lora_attention(pretrain_model, r=8, alpha=16)\n",
    "lora_state = torch.load(\"lora_sft.pt\")\n",
    "base_model.load_state_dict(lora_state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ed7fc83-01b1-47ae-ba46-5c3994f61372",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04b60c7d-3dae-40a7-b1d0-865fc413b32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(clean_state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "563ec88b-c933-41ef-8a59-de4d23dc8246",
   "metadata": {},
   "outputs": [],
   "source": [
    "inject_lora_attention(model, r=8, alpha=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8adec34-9dd0-4e2d-b6e4-697f761dedf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['decoder.causal_mask', 'decoder.embedding.weight', 'decoder.layers.0.swi_glu.w1.weight', 'decoder.layers.0.swi_glu.w2.weight', 'decoder.layers.0.swi_glu.w3.weight', 'decoder.layers.0.masked_mha.Q.weight', 'decoder.layers.0.masked_mha.Q.bias', 'decoder.layers.0.masked_mha.K.weight', 'decoder.layers.0.masked_mha.K.bias', 'decoder.layers.0.masked_mha.V.weight', 'decoder.layers.0.masked_mha.V.bias', 'decoder.layers.0.masked_mha.fc_out.weight', 'decoder.layers.0.masked_mha.fc_out.bias', 'decoder.layers.0.rms_norm0.weight', 'decoder.layers.0.rms_norm1.weight', 'decoder.layers.1.swi_glu.w1.weight', 'decoder.layers.1.swi_glu.w2.weight', 'decoder.layers.1.swi_glu.w3.weight', 'decoder.layers.1.masked_mha.Q.weight', 'decoder.layers.1.masked_mha.Q.bias', 'decoder.layers.1.masked_mha.K.weight', 'decoder.layers.1.masked_mha.K.bias', 'decoder.layers.1.masked_mha.V.weight', 'decoder.layers.1.masked_mha.V.bias', 'decoder.layers.1.masked_mha.fc_out.weight', 'decoder.layers.1.masked_mha.fc_out.bias', 'decoder.layers.1.rms_norm0.weight', 'decoder.layers.1.rms_norm1.weight', 'decoder.layers.2.swi_glu.w1.weight', 'decoder.layers.2.swi_glu.w2.weight', 'decoder.layers.2.swi_glu.w3.weight', 'decoder.layers.2.masked_mha.Q.weight', 'decoder.layers.2.masked_mha.Q.bias', 'decoder.layers.2.masked_mha.K.weight', 'decoder.layers.2.masked_mha.K.bias', 'decoder.layers.2.masked_mha.V.weight', 'decoder.layers.2.masked_mha.V.bias', 'decoder.layers.2.masked_mha.fc_out.weight', 'decoder.layers.2.masked_mha.fc_out.bias', 'decoder.layers.2.rms_norm0.weight', 'decoder.layers.2.rms_norm1.weight', 'decoder.layers.3.swi_glu.w1.weight', 'decoder.layers.3.swi_glu.w2.weight', 'decoder.layers.3.swi_glu.w3.weight', 'decoder.layers.3.masked_mha.Q.weight', 'decoder.layers.3.masked_mha.Q.bias', 'decoder.layers.3.masked_mha.K.weight', 'decoder.layers.3.masked_mha.K.bias', 'decoder.layers.3.masked_mha.V.weight', 'decoder.layers.3.masked_mha.V.bias', 'decoder.layers.3.masked_mha.fc_out.weight', 'decoder.layers.3.masked_mha.fc_out.bias', 'decoder.layers.3.rms_norm0.weight', 'decoder.layers.3.rms_norm1.weight', 'decoder.layers.4.swi_glu.w1.weight', 'decoder.layers.4.swi_glu.w2.weight', 'decoder.layers.4.swi_glu.w3.weight', 'decoder.layers.4.masked_mha.Q.weight', 'decoder.layers.4.masked_mha.Q.bias', 'decoder.layers.4.masked_mha.K.weight', 'decoder.layers.4.masked_mha.K.bias', 'decoder.layers.4.masked_mha.V.weight', 'decoder.layers.4.masked_mha.V.bias', 'decoder.layers.4.masked_mha.fc_out.weight', 'decoder.layers.4.masked_mha.fc_out.bias', 'decoder.layers.4.rms_norm0.weight', 'decoder.layers.4.rms_norm1.weight', 'decoder.layers.5.swi_glu.w1.weight', 'decoder.layers.5.swi_glu.w2.weight', 'decoder.layers.5.swi_glu.w3.weight', 'decoder.layers.5.masked_mha.Q.weight', 'decoder.layers.5.masked_mha.Q.bias', 'decoder.layers.5.masked_mha.K.weight', 'decoder.layers.5.masked_mha.K.bias', 'decoder.layers.5.masked_mha.V.weight', 'decoder.layers.5.masked_mha.V.bias', 'decoder.layers.5.masked_mha.fc_out.weight', 'decoder.layers.5.masked_mha.fc_out.bias', 'decoder.layers.5.rms_norm0.weight', 'decoder.layers.5.rms_norm1.weight', 'decoder.layers.6.swi_glu.w1.weight', 'decoder.layers.6.swi_glu.w2.weight', 'decoder.layers.6.swi_glu.w3.weight', 'decoder.layers.6.masked_mha.Q.weight', 'decoder.layers.6.masked_mha.Q.bias', 'decoder.layers.6.masked_mha.K.weight', 'decoder.layers.6.masked_mha.K.bias', 'decoder.layers.6.masked_mha.V.weight', 'decoder.layers.6.masked_mha.V.bias', 'decoder.layers.6.masked_mha.fc_out.weight', 'decoder.layers.6.masked_mha.fc_out.bias', 'decoder.layers.6.rms_norm0.weight', 'decoder.layers.6.rms_norm1.weight', 'decoder.layers.7.swi_glu.w1.weight', 'decoder.layers.7.swi_glu.w2.weight', 'decoder.layers.7.swi_glu.w3.weight', 'decoder.layers.7.masked_mha.Q.weight', 'decoder.layers.7.masked_mha.Q.bias', 'decoder.layers.7.masked_mha.K.weight', 'decoder.layers.7.masked_mha.K.bias', 'decoder.layers.7.masked_mha.V.weight', 'decoder.layers.7.masked_mha.V.bias', 'decoder.layers.7.masked_mha.fc_out.weight', 'decoder.layers.7.masked_mha.fc_out.bias', 'decoder.layers.7.rms_norm0.weight', 'decoder.layers.7.rms_norm1.weight', 'decoder.layers.8.swi_glu.w1.weight', 'decoder.layers.8.swi_glu.w2.weight', 'decoder.layers.8.swi_glu.w3.weight', 'decoder.layers.8.masked_mha.Q.weight', 'decoder.layers.8.masked_mha.Q.bias', 'decoder.layers.8.masked_mha.K.weight', 'decoder.layers.8.masked_mha.K.bias', 'decoder.layers.8.masked_mha.V.weight', 'decoder.layers.8.masked_mha.V.bias', 'decoder.layers.8.masked_mha.fc_out.weight', 'decoder.layers.8.masked_mha.fc_out.bias', 'decoder.layers.8.rms_norm0.weight', 'decoder.layers.8.rms_norm1.weight', 'decoder.layers.9.swi_glu.w1.weight', 'decoder.layers.9.swi_glu.w2.weight', 'decoder.layers.9.swi_glu.w3.weight', 'decoder.layers.9.masked_mha.Q.weight', 'decoder.layers.9.masked_mha.Q.bias', 'decoder.layers.9.masked_mha.K.weight', 'decoder.layers.9.masked_mha.K.bias', 'decoder.layers.9.masked_mha.V.weight', 'decoder.layers.9.masked_mha.V.bias', 'decoder.layers.9.masked_mha.fc_out.weight', 'decoder.layers.9.masked_mha.fc_out.bias', 'decoder.layers.9.rms_norm0.weight', 'decoder.layers.9.rms_norm1.weight', 'decoder.layers.10.swi_glu.w1.weight', 'decoder.layers.10.swi_glu.w2.weight', 'decoder.layers.10.swi_glu.w3.weight', 'decoder.layers.10.masked_mha.Q.weight', 'decoder.layers.10.masked_mha.Q.bias', 'decoder.layers.10.masked_mha.K.weight', 'decoder.layers.10.masked_mha.K.bias', 'decoder.layers.10.masked_mha.V.weight', 'decoder.layers.10.masked_mha.V.bias', 'decoder.layers.10.masked_mha.fc_out.weight', 'decoder.layers.10.masked_mha.fc_out.bias', 'decoder.layers.10.rms_norm0.weight', 'decoder.layers.10.rms_norm1.weight', 'decoder.layers.11.swi_glu.w1.weight', 'decoder.layers.11.swi_glu.w2.weight', 'decoder.layers.11.swi_glu.w3.weight', 'decoder.layers.11.masked_mha.Q.weight', 'decoder.layers.11.masked_mha.Q.bias', 'decoder.layers.11.masked_mha.K.weight', 'decoder.layers.11.masked_mha.K.bias', 'decoder.layers.11.masked_mha.V.weight', 'decoder.layers.11.masked_mha.V.bias', 'decoder.layers.11.masked_mha.fc_out.weight', 'decoder.layers.11.masked_mha.fc_out.bias', 'decoder.layers.11.rms_norm0.weight', 'decoder.layers.11.rms_norm1.weight', 'decoder.norm.weight', 'lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_state = torch.load(\"lora_sft.pt\")\n",
    "model.load_state_dict(lora_state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "142a43fc-28a1-4caa-9c28-07de5cbae5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.layers.0.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.0.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.0.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.0.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.0.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.0.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.1.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.1.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.1.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.1.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.1.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.1.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.2.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.2.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.2.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.2.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.2.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.2.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.3.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.3.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.3.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.3.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.3.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.3.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.4.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.4.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.4.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.4.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.4.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.4.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.5.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.5.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.5.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.5.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.5.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.5.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.6.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.6.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.6.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.6.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.6.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.6.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.7.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.7.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.7.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.7.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.7.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.7.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.8.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.8.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.8.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.8.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.8.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.8.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.9.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.9.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.9.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.9.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.9.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.9.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.10.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.10.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.10.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.10.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.10.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.10.masked_mha.V.lora_B.weight True\n",
      "decoder.layers.11.masked_mha.Q.lora_A.weight True\n",
      "decoder.layers.11.masked_mha.Q.lora_B.weight True\n",
      "decoder.layers.11.masked_mha.K.lora_A.weight True\n",
      "decoder.layers.11.masked_mha.K.lora_B.weight True\n",
      "decoder.layers.11.masked_mha.V.lora_A.weight True\n",
      "decoder.layers.11.masked_mha.V.lora_B.weight True\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    if \"lora_\" in n:\n",
    "        print(n, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd8bbfe-9980-41a6-a1a5-55688966e9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (K): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (V): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64702e69-8939-4885-951b-04aa27594f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_GPT_model(\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(32768, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x Decoder_GPT_Block(\n",
       "        (swi_glu): SwiGLU_FFN(\n",
       "          (w1): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w2): Linear(in_features=512, out_features=1536, bias=False)\n",
       "          (w3): Linear(in_features=1536, out_features=512, bias=False)\n",
       "          (act): SiLU()\n",
       "        )\n",
       "        (masked_mha): Masked_MHA(\n",
       "          (Q): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (K): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (V): LoRALinear(\n",
       "            (lora_A): Linear(in_features=512, out_features=8, bias=False)\n",
       "            (lora_B): Linear(in_features=8, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (rms_norm0): RMSNorm()\n",
       "        (rms_norm1): RMSNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d34fe832-8f08-4596-b971-5bf0ce2e7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_filtering(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    probs = torch.softmax(sorted_logits, dim=-1)\n",
    "\n",
    "    cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "\n",
    "    # tokens remove where cumulative prob > top_p\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[indices_to_remove] = -float(\"inf\")\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cacb38f-bb16-4b2a-801a-1388d1b4da0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS: 2 EOS: 3 PAD: 0\n",
      "Loading checkpoint...\n",
      "Initializing model...\n",
      "Model loaded successfully!\n",
      "Prompt: ### प्रश्न:\n",
      "प्रकृति के बारे में एक ऐसी कविता बनाएँ जिसमें केवल दो अलग-अलग तुकबंदी शब्दों का उपयोग हो।\n",
      "\n",
      "### उत्तर:\n",
      "\n",
      "--- Generation 1 ---\n",
      " ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "------------------------------------------------------------\n",
      "--- Generation 2 ---\n",
      " ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "------------------------------------------------------------\n",
      "--- Generation 3 ---\n",
      " ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "from decoder_only_gpt import My_GPT_model_SFT\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CKPT_PATH = \"checkpoints_sft_step_140000.pt\"\n",
    "SEQ_LEN = 512\n",
    "\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 0.85\n",
    "REPETITION_PENALTY = 1.7\n",
    "MAX_NEW_TOKENS = 80\n",
    "\n",
    "sp = spm.SentencePieceProcessor(model_file=\"hindi_tokenizer_new.model\")\n",
    "print(\"BOS:\", sp.bos_id(), \"EOS:\", sp.eos_id(), \"PAD:\", sp.pad_id())\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(model, input_ids, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_p=TOP_P, repetition_penalty=REPETITION_PENALTY):\n",
    "    model.eval()\n",
    "    eos_id = sp.eos_id()\n",
    "    seq_len = input_ids.shape[1]\n",
    "\n",
    "    for step in range(max_new_tokens):\n",
    "        # Keep only last SEQ_LEN tokens\n",
    "        input_cond = input_ids[:, -SEQ_LEN:]\n",
    "        logits = model(input_cond)  # (B, T, V)\n",
    "        logits = logits[:, -1, :] / temperature  # Take last token logits & apply temperature\n",
    "\n",
    "        # Clamp logits to avoid extreme values\n",
    "        logits = torch.clamp(logits, min=-1e9, max=1e9)\n",
    "\n",
    "        # Penalize non-Devanagari tokens\n",
    "        for tid in range(sp.get_piece_size()):\n",
    "            piece = sp.id_to_piece(tid)\n",
    "            if not any('\\u0900' <= ch <= '\\u097F' for ch in piece):\n",
    "                logits[0, tid] -= 5.0\n",
    "\n",
    "        # Apply repetition penalty on generated tokens only (not prompt)\n",
    "        generated_tokens = input_ids[0, seq_len:].tolist()\n",
    "        for t in set(generated_tokens):\n",
    "            logits[0, t] /= repetition_penalty\n",
    "\n",
    "        # Top-p (nucleus) sampling\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        mask = cum_probs > top_p\n",
    "        mask[..., 1:] = mask[..., :-1].clone()\n",
    "        mask[..., 0] = False\n",
    "\n",
    "        sorted_probs[mask] = 0.0\n",
    "        sorted_probs /= sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        next_token = torch.multinomial(sorted_probs, 1)\n",
    "        next_token = torch.gather(sorted_indices, -1, next_token)\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "        if next_token.item() == eos_id:\n",
    "            break\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Loading checkpoint...\")\n",
    "    ckpt = torch.load(CKPT_PATH, map_location=DEVICE)\n",
    "\n",
    "    print(\"Initializing model...\")\n",
    "    model = My_GPT_model_SFT(\n",
    "        vocab_size=sp.get_piece_size(),\n",
    "        num_layers=12,\n",
    "        d_model=512,\n",
    "        d_ff=2048,\n",
    "        num_heads=8,\n",
    "        seq_len=SEQ_LEN\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    state_dict = ckpt.get(\"model\", ckpt)\n",
    "    if any(k.startswith(\"_orig_mod.\") for k in state_dict.keys()):\n",
    "        print(\"Detected torch.compile prefix, stripping '_orig_mod.'...\")\n",
    "        state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    prompt = \"### प्रश्न:\\nप्रकृति के बारे में एक ऐसी कविता बनाएँ जिसमें केवल दो अलग-अलग तुकबंदी शब्दों का उपयोग हो।\\n\\n### उत्तर:\\n\"\n",
    "    # prompt = \"### प्रश्न:\\nशिविर यात्रा के लिए एक व्यक्ति को दस वस्तुओं की आवश्यकता हो सकती हैः\\n\\n1. तम्बू-तत्वों से आश्रय और सुरक्षा प्रदान करने के लिए\\n2. स्लीपिंग बैग-सोते समय गर्म और आरामदायक रहने के लिए\\n3. पोर्टेबल स्टोव या कैम्पफायर ग्रिल-खाना पकाने के लिए\\n4. जल्दी खराब होने वाले भोजन और पेय को ठंडा रखने के लिए बर्फ या बर्फ के पैक के साथ ठंडा करें।\\n5. लालटेन या टॉर्च-रात के दौरान प्रकाश प्रदान करने के लिए\\n6. प्राथमिक चिकित्सा किट-मामूली चोटों या बीमारियों के लिए\\n7. मानचित्र और कम्पास या जी. पी. एस.-पर्वतारोहण या क्षेत्र की खोज के लिए\\n8. कैम्प कुर्सियाँ या तह कुर्सियाँ-कैम्पसाइट के आसपास आरामदायक बैठने के लिए\\n9. कीट विकर्षक-कीट के काटने से बचाने के लिए\\n10. सनस्क्रीन-सनबर्न से बचाने के लिए\\n\\n### उत्तर:\\n\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "\n",
    "    input_ids = [sp.bos_id()] + sp.encode(prompt, out_type=int)\n",
    "    input_ids = torch.tensor([input_ids], device=DEVICE)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"--- Generation {i+1} ---\")\n",
    "        output_ids = generate(model, input_ids.clone())\n",
    "        generated_tokens = output_ids[0, input_ids.shape[1]:].tolist()\n",
    "        generated_text = sp.decode(generated_tokens)\n",
    "        print(generated_text)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "689888a4-ca59-4590-818b-4c2b90120796",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "generate() got an unexpected keyword argument 'eos_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m PENALTY_WINDOW \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m       \u001b[38;5;66;03m# Not used directly now, but kept for future\u001b[39;00m\n\u001b[0;32m     20\u001b[0m MAX_NEW_TOKENS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m---> 22\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(sp\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate() got an unexpected keyword argument 'eos_token_id'"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor(model_file=\"hindi_tokenizer_new.model\")\n",
    "import torch.nn.functional as F\n",
    "\n",
    "prompt = \"भारत का भविष्य\"\n",
    "input_ids = torch.tensor(\n",
    "    [sp.encode(prompt)],\n",
    "    dtype=torch.long\n",
    ")\n",
    "\n",
    "SEQ_LEN = 512\n",
    "\n",
    "TEMPERATURE = 0.6\n",
    "TOP_P = 0.85\n",
    "REPETITION_PENALTY = 1.6\n",
    "\n",
    "# Sampling hyperparameters\n",
    "  # ← Increase this! 1.2–1.8 works best for repetitive small models\n",
    "PENALTY_WINDOW = 128       # Not used directly now, but kept for future\n",
    "MAX_NEW_TOKENS = 200\n",
    "\n",
    "output_ids = generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    max_new_tokens=80,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=sp.eos_id()\n",
    ")\n",
    "\n",
    "print(sp.decode(output_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f83aa-0ecd-47e2-9841-fbccf109ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"checkpoints_HindiGPT-v1_step280000.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12379dc8-d7a8-4349-b9d0-965c82804234",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ckpt[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b2dbc-e6bb-4626-9d72-fcde9316df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"checkpoints_HindiGPT-v1_step280000.pt\", map_location=\"cpu\")\n",
    "print(ckpt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd479d-c856-4b1c-b69e-5b1207a9f6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_state = ckpt[\"model\"]\n",
    "\n",
    "clean_state = {}\n",
    "for k, v in raw_state.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        k = k.replace(\"_orig_mod.\", \"\")\n",
    "    clean_state[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80658568-1765-4e67-b34c-2c76747d84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = ckpt[\"model\"]\n",
    "print(list(clean_state.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefa88b-d5a6-4c82-b170-3bc3a2bfee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_state_dict(clean_state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ff882-c56b-49d3-90b8-b0ee1d990067",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "TABLE_PREFIX = \"com_\"\n",
    "table = f\"{TABLE_PREFIX}{i}\"\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774f013-d80c-40dc-bee2-7cb542646394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
