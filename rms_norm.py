import torch
from torch import nn

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        rms = x.pow(2).mean(-1, keepdim=True).sqrt()
        RMS_norm = x / (rms + self.eps) * self.weight
        return RMS_norm
    


# âœ… LayerNorm vs RMSNorm â€” Why RMSNorm is preferred in LLMs
# 1. LayerNorm formula

# LN normalize both mean and variance:

# LN(ğ‘¥) = ((ğ‘¥âˆ’ğœ‡) / (ğœ^2 + ğœ–)^0.5) * ğ›¾ + ğ›½

# Mean subtract hoti hai â†’ zero-centered output
# Variance normalize hoti hai
# Bias vector (beta) bhi learn hota hai

# 2. RMSNorm formula
# RMSNorm mean subtract nahi karta:

# RMSNorm(ğ‘¥) = (ğ‘¥ / (((1/ğ‘‘) * âˆ‘ğ‘¥^2) + ğœ–)) * ğ›¾

# No mean-centering
# Sirf RMS scale normalize hota hai (variance part)